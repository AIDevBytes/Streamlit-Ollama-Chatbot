import ollama
from config import Config

def chat(user_prompt, model):
    ...

# handles stream response back from LLM
    
def stream_parser(stream):
    ...